<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>EXPAND</title>
    <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&amp;display=swap">
    <link rel="stylesheet" href="assets/css/Footer-Dark.css">
    <link rel="stylesheet" href="assets/css/Header-Dark.css">
    <link rel="stylesheet" href="assets/css/styles.css">
</head>

<body>
    <div style="background: url(&quot;assets/img/mountain_bg.jpg&quot;);">
        <div class="container hero">
            <div class="row">
                <div class="col">
                    <p style="color: rgb(255,255,255);font-family: Roboto, sans-serif;padding-top: 15px;">NeurIPS 2021 Spotlight</p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <h1 class="text-center" style="margin: 0px;font-family: Roboto, sans-serif;color: rgb(255,255,255);margin-top: 5px;">Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation</h1>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p style="font-family: Roboto, sans-serif;color: rgb(255,255,255);text-align: center;font-size: 15px;margin: 4px;margin-top: 12px;">Lin Guan<sup>1</sup>,&nbsp; Mudit Verma<sup>1</sup>,&nbsp; Sihang Guo<sup>2</sup>,&nbsp; Ruohan Zhang<sup>3</sup>,&nbsp; Subbarao Kambhampati<sup>1</sup><br></p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p style="font-family: Roboto, sans-serif;color: rgb(255,255,255);text-align: center;font-size: 13px;margin: 5px;margin-bottom: 26px;margin-top: 10px;"><sup>1</sup>School of Computing &amp; AI,&nbsp; Arizona State University<br><sup>2</sup>Department of Computer Science,&nbsp; The University of Texas at Austin<br><sup>3</sup>Department of Computer Science,&nbsp; Stanford University<br></p>
                </div>
            </div>
        </div>
    </div>
    <div>
        <div class="container">
            <div class="row">
                <div class="col"><img class="center-block" src="assets/img/expand.png" style="height: 300px;"></div>
            </div>
        </div>
    </div>
    <div>
        <div class="container">
            <div class="row">
                <div class="col">
                    <h1 style="margin-top: 15px;font-family: Roboto, sans-serif;font-size: 25px;color: #333;margin-left: 24px;">Abstract</h1>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p style="font-family: Roboto, sans-serif;margin-left: 24px;margin-right: 24px;margin-top: 2px;font-size: 14px;color: #333;">Human explanation (e.g., in terms of feature importance) has been recently used to extend the communication channel between human and agent in interactive machine learning. Under this setting, human trainers provide not only the ground truth but also some form of explanation. However, this kind of human guidance was only investigated in supervised learning tasks, and it remains unclear how to best incorporate this type of human knowledge into deep reinforcement learning. In this paper, we present the first study of using human visual explanations in human-in-the-loop reinforcement learning (HIRL). We focus on the task of learning from feedback, in which the human trainer not only gives binary evaluative "good" or "bad" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the model to encode task-relevant features through a context-aware data augmentation that only perturbs irrelevant features in human salient information. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate the performance and sample efficiency of this approach. We show that our method significantly outperforms methods leveraging human explanation that are adapted from supervised learning, and Human-in-the-loop RL baselines that only utilize evaluative feedback.</p>
                </div>
            </div>
        </div>
    </div>
    <div>
        <div class="container">
            <div class="row">
                <div class="col">
                    <h1 style="margin-top: 15px;font-family: Roboto, sans-serif;font-size: 25px;color: #333;margin-left: 24px;">Resource</h1>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p style="margin-left: 23px;"><a href="https://arxiv.org/abs/2006.14804">Paper</a>&nbsp; |&nbsp;&nbsp;<a href="https://recorder-v3.slideslive.com/?share=50811&amp;s=dad4cbb4-9ea8-4790-b33c-beb3a0b5739e">Talk (Google Chrome Recommended)</a></p>
                </div>
            </div>
        </div>
    </div>
    <div>
        <footer class="footer-dark" style="height: 50px;margin-top: 25px;">
            <div class="container">
                <p class="copyright" style="padding-top: 0px;">Yochan Lab Â© 2021</p>
            </div>
        </footer>
    </div>
    <script src="assets/bootstrap/js/bootstrap.min.js"></script>
</body>

</html>