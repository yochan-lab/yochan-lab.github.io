<!DOCTYPE html>

<html lang="en">

<head>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="icon" href="../../favicon.ico">

    <title>Yochan @ ICML 2023</title>

    <!-- Bootstrap core CSS -->
    <link href="../../ASSETS/bootstrap-4.0.0/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./../files/css/custom.css" rel="stylesheet">

    <!-- jQuery -->
    <script src="../../ASSETS/jquery/jquery-3.2.1.min.js"></script>

    <!-- Bootstrap core JavaScript
        ================================================== -->
    <script src="../../ASSETS/bootstrap-4.0.0/js/popper.min.js"></script>
    <script src="../../ASSETS/bootstrap-4.0.0/js/bootstrap.min.js"></script>

    <!-- Custom JavaScript -->
    <script src="./../files/js/custom.js"></script>

</head>

<body class="bg-light" id="body" data-spy="scroll" data-target="#body" data-offset="100">

    <nav class="navbar navbar-expand-md fixed-top navbar-dark bg-dark">
        <li class="navbar-brand spy-enabled text-light"><strong>Yochan @ ICML 2023</strong></li>
    </nav>

    <div class="nav-scroller bg-white box-shadow">
        <nav class="nav nav-underline">
            <a class="nav-link bold" href="http://rakaposhi.eas.asu.edu/yochan/">Yochan</a>
            <a class="nav-link bold" href="https://robotics.asu.edu/">ASU Robotics</a>
            <a class="nav-link nav-link-no-link d-none d-sm-block">Last Updated <span
                    class="badge badge-pill bg-light align-text-bottom">2023-07-27</span></a>
        </nav>
    </div>

    <main role="main" class="container">

        <div class="d-flex align-items-center p-3 my-3 text-white-50 bg-maroon rounded box-shadow">
            <img class="mr-3" src="./../files/images/icon.png" width="48" height="48" style="border-radius: 3pt;">
            <div class="lh-100">
                <h6 class="mb-0 text-white lh-100">Yochan Lab | Arizona State University</h6>
                <small>PI: Subbarao Kambhampati | <a class="nav-link-yellow text-white-50"
                        href="mailto:rao@asu.edu">rao@asu.edu</a> | <a class="nav-link-yellow text-white-50"
                        href="https://twitter.com/rao2z">@rao2z</a> | <a class="nav-link-yellow text-white-50"
                        href="http://rakaposhi.eas.asu.edu/">rakaposhi.eas.asu.edu</a> </small>
            </div>
        </div>

        <div class="my-3 p-3 bg-white rounded box-shadow" id="AAAI-2020">

            <h6 class="border-bottom border-gray pb-2 mb-0">ICML 2023</h6>

            <div id="AAAI-2019">

                <div id="accordion-AAAI-2019" role="tablist">

                    <div>
                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-3-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-3-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Invited Talk: Avenging Polanyi's Revenge: Exploiting
                                        the Approximate Omniscience of LLMs in Planning without Deluding Yourself In the
                                        Process</strong>
                                </div>
                                <span class="d-block">Subbarao Kambhampati</span>
                                <br>
                                <span class="d-block"><em>KLR Workshop on Friday, July 28th. Meeting Room
                                        301.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://klr-icml2023.github.io/schedule.html" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-3-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    LLMs are on track to reverse what seemed like an inexorable shift of AI from
                                    explicit to tacit knowledge tasks. Trained as they are on everything ever written on
                                    the web, LLMs exhibit "approximate omniscience"--they can provide answers to all
                                    sorts of queries, with nary a guarantee. This could herald a new era for
                                    knowledge-based AI systems--with LLMs taking the role of (blowhard?) experts. But
                                    first, we have to stop confusing the impressive form of the generated knowledge for
                                    correct content, and resist the temptatuon to ascribe reasoning powers to
                                    approximate retrieval by these n-gram models on steroids. We have to focus instead
                                    on LLM-Modulo techniques that complement the unfettered idea generation of LLMs with
                                    careful vetting by model-based AI systems. In this talk, I will reify this vision
                                    and attendant caveats in the context of the role of LLMs in planning tasks.
                                </p>
                            </div>
                            <hr>
                        </div>
                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-1-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-1-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">On the Planning Abilities of Large Language Models --
                                        A Critical Investigation</strong>
                                </div>
                                <span class="d-block">Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao
                                    Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>KLR Workshop on Friday, July 28th. Meeting Room
                                        301.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://arxiv.org/abs/2305.15771" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-1-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
                                    general web corpora, in this paper, we set out to investigate their planning
                                    capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans
                                    autonomously in commonsense planning tasks and (2) the potential of LLMs as a source
                                    of heuristic guidance for other agents (AI planners) in their planning tasks. We
                                    conduct a systematic study by generating a suite of instances on domains similar to
                                    the ones employed in the International Planning Competition and evaluate LLMs in two
                                    distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to
                                    generate executable plans autonomously is rather limited, with the best model
                                    (GPT-4) having an average success rate of ~12% across the domains. However, the
                                    results in the heuristic mode show more promise. In the heuristic mode, we
                                    demonstrate that LLM-generated plans can improve the search process for underlying
                                    sound planners and additionally show that external verifiers can help provide
                                    feedback on the generated plans and back-prompt the LLM for better plan generation.
                                </p>
                            </div>
                            <hr>
                        </div>

                    </div>
                    <div>

                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-2-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-2-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Leveraging Pre-trained Large Language Models to
                                        Construct and Utilize World Models for Model-based Task Planning</strong>
                                </div>
                                <span class="d-block">Lin Guan*, Karthik Valmeekam*, Sarath Sreedharan, Subbarao
                                    Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>KLR Workshop on Friday, July 28th. Meeting Room
                                        301.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://arxiv.org/abs/2305.14909" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-2-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    There is a growing interest in applying pre-trained large language models (LLMs) to
                                    planning problems. However, methods that use LLMs directly as planners are currently
                                    impractical due to several factors, including limited correctness of plans, strong
                                    reliance on feedback from interactions with simulators or even the actual
                                    environment, and the inefficiency in utilizing human feedback. In this work, we
                                    introduce a novel alternative paradigm that constructs an explicit world (domain)
                                    model in planning domain definition language (PDDL) and then uses it to plan with
                                    sound domain-independent planners. To address the fact that LLMs may not generate a
                                    fully functional PDDL model initially, we employ LLMs as an interface between PDDL
                                    and sources of corrective feedback, such as PDDL validators and humans. For users
                                    who lack a background in PDDL, we show that LLMs can translate PDDL into natural
                                    language and effectively encode corrective feedback back to the underlying domain
                                    model. Our framework not only enjoys the correctness guarantee offered by the
                                    external planners but also reduces human involvement by allowing users to correct
                                    domain models at the beginning, rather than inspecting and correcting (through
                                    interactive prompting) every generated plan as in previous work. On two IPC domains
                                    and a Household domain that is more complicated than commonly used benchmarks such
                                    as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL
                                    models for over 40 actions, and the corrected PDDL models are then used to
                                    successfully solve 48 challenging planning tasks. Resources including the source
                                    code will be released at: this https URL.
                                </p>
                            </div>
                            <hr>
                        </div>

                    </div>
                    <div>
                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-6-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-6-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Learning and Leveraging Verifiers to Improve Planning
                                        Capabilities of Pre-trained Language Models</strong>
                                </div>
                                <span class="d-block">Daman Arora, Subbarao Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>KLR Workshop on Friday, July 28th. Meeting Room
                                        301.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://arxiv.org/abs/2305.17077" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-6-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    There have been wide spread claims in the literature about the emergent reasoning
                                    capabilities of Pretrained Large Language Models. However, recent studies, have
                                    found that their ability to plan remains questionable. Through our experiments using
                                    GPT-2, we empirically demonstrate that the performance of a finetuned baseline
                                    remains poor because it violates pre-conditions of actions in the plans that it
                                    generates. To improve the planning capabilities of a finetuned LLM, we train a
                                    verifier, which can classify actions as being valid or invalid in a particular
                                    state. By randomly sampling actions from the same dataset, we generate examples of
                                    invalid actions which are then used to train a verifier which can check for action
                                    applicability. In the presence of diverse sampling from a generator and a verifier
                                    which can prune invalid trajectories, we show significant gains in the success rate
                                    on the Blocksworld domain. Additionally, we show that finetuning the GPT-2 generator
                                    itself to create the verifier generalizes better than finetuning the base GPT-2.
                                    Lastly, we investigate the role of the sampling temperature which can be used to
                                    control the exploration-exploitation tradeoff.

                                </p>
                            </div>
                            <hr>
                        </div>
                        

                    </div>
                    <div>

                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-4-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-4-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Preference Proxies: Evaluating Large Language Models
                                        in capturing Human Preferences in Human-AI Tasks</strong>
                                </div>
                                <span class="d-block">Mudit Verma*, Siddhant Bhambri*, Subbarao Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>Theory-of-Mind Workshop on Friday, July 28th. Meeting Room
                                        317A.</em><br><em>Many Facets of Preference-Based Learning Workshop on Friday,
                                        July 28th. Meeting Room 316AB.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://openreview.net/pdf?id=m6EpkjUUBR" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-4-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    In this work, we investigate the potential of Large
                                    Language Models (LLMs) to serve as effective
                                    human proxies by capturing human preferences
                                    in the context of collaboration with AI agents. Focusing on two key aspects of human
                                    preferences
                                    - explicability and sub-task specification in team
                                    settings - we explore LLMs’ ability to not only
                                    model mental states but also understand human
                                    reasoning processes. By developing scenarios
                                    where optimal AI performance relies on modeling
                                    human mental states and reasoning, our investigation involving two different
                                    preference types
                                    and a user study (with 17 participants) contributes
                                    valuable insights into the suitability of LLMs as
                                    “Preference Proxies” in various human-AI applications, paving the way for future
                                    research on
                                    the integration of AI agents with human users in
                                    Human-Aware AI tasks
                                </p>
                            </div>
                            <hr>
                        </div>

                    </div>
                    <div>

                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-5-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-5-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Exploiting Action Distances for Reward Learning from
                                        Human Preferences</strong>
                                </div>
                                <span class="d-block">Mudit Verma, Siddhant Bhambri, Subbarao Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>Many Facets of Preference-Based Learning Workshop on Friday,
                                        July 28th. Meeting Room 316AB.</em></span>
                                <a class="paper-link text-right badge badge-warning" href="None"
                                    target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-5-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    [Abstract]
                                </p>
                            </div>
                            <hr>
                        </div>

                    </div>
                    <div>

                        

                        <div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-7-AAAI-2019"
                            aria-expanded="false" aria-controls="collapse-6-AAAI-2019">
                            <div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
                                <div class="d-flex justify-content-between align-items-center w-100">
                                    <strong class="text-gray-dark">Relative Behavioral Attributes: Filling the Gap
                                        between Symbolic Goal Specification and Reward Learning from Human
                                        Preferences</strong>
                                </div>
                                <span class="d-block">Lin Guan, Karthik Valmeekam & Subbarao Kambhampati.</span>
                                <br>
                                <span class="d-block"><em>Interactive Learning with Implicit Human Feedback Workshop on
                                        Saturday, July 29th. Meeting Room 315.</em></span>
                                <a class="paper-link text-right badge badge-warning"
                                    href="https://openreview.net/pdf?id=lGz9u1ubUXE" target="_blank">Link</a>

                            </div>

                        </div>

                        <div id="collapse-7-AAAI-2019" class="collapse" role="tabpanel"
                            data-parent="#accordion-AAAI-2019">
                            <div class="card-body">
                                <p style="color:black;">
                                    Generating complex behaviors that satisfy the preferences of non-expert users
                                    is a crucial requirement for AI agents. Interactive reward learning from trajectory
                                    comparisons (a.k.a. RLHF) is one way to allow non-expert users to convey
                                    complex objectives by expressing preferences over short clips of agent behaviors.
                                    Even though this parametric method can encode complex tacit knowledge
                                    present in the underlying tasks, it implicitly assumes that the human is unable
                                    to provide richer feedback than binary preference labels, leading to intolerably
                                    high feedback complexity and poor user experience. While providing a detailed
                                    symbolic closed-form specification of the objectives might be tempting, it is not
                                    always feasible even for an expert user. However, in most cases, humans are
                                    aware of how the agent should change its behavior along meaningful axes to fulfill
                                    their underlying purpose, even if they are not able to fully specify task objectives
                                    symbolically. Using this as motivation, we introduce the notion of Relative
                                    Behavioral Attributes, which allows the users to tweak the agent behavior through
                                    symbolic concepts (e.g., increasing the softness or speed of agents’ movement).
                                    We propose two practical methods that can learn to model any kind of behavioral
                                    attributes from ordered behavior clips. We demonstrate the effectiveness of our
                                    methods on four tasks with nine different behavioral attributes, showing that once
                                    the attributes are learned, end users can produce desirable agent behaviors
                                    relatively effortlessly, by providing feedback just around ten times. This is over
                                    an
                                    order of magnitude less than that required by the popular learning-from-human
                                    preferences baselines. The supplementary video and source code are available at:
                                    <a href="https://guansuns.github.io/pages/rba"
                                        target="_blank">https://guansuns.github.io/pages/rba</a>

                                </p>
                            </div>
                            <hr>
                        </div>

                    </div>


                </div>

            </div>

        </div>


    </main>

    <div class="footer-container">

        <div class="footer-bottom-left" style="width:100%;">

            <hr>

            <footer class="text-center" style="color:gray;">
                <p>&copy; Yochan</p>
            </footer>

        </div>

    </div>

</body>

</html>